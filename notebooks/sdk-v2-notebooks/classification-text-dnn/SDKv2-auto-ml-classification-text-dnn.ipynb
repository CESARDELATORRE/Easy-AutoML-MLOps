{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/classification-text-dnn/auto-ml-classification-text-dnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Machine Learning\n",
    "_**Text Classification Using Deep Learning**_\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "1. [Evaluate](#Evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook demonstrates classification with text data using deep learning in AutoML.\n",
    "\n",
    "AutoML highlights here include using deep neural networks (DNNs) to create embedded features from text data. Depending on the compute cluster the user provides, AutoML tried out Bidirectional Encoder Representations from Transformers (BERT) when a GPU compute is used, and Bidirectional Long-Short Term neural network (BiLSTM) when a CPU compute is used, thereby optimizing the choice of DNN for the uesr's setup.\n",
    "\n",
    "Make sure you have executed the [configuration](../../../configuration.ipynb) before running this notebook.\n",
    "\n",
    "Notebook synopsis:\n",
    "\n",
    "1. Creating an Experiment in an existing Workspace\n",
    "2. Configuration and remote run of AutoML for a text dataset (20 Newsgroups dataset from scikit-learn) for classification\n",
    "3. Registering the best model for future use\n",
    "4. Evaluating the final model on a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n",
      "\"Dataset.get\" is deprecated after version 1.0.69. Please use \"Dataset.get_by_name\" and \"Dataset.get_by_id\" to retrieve dataset. See Dataset API change notice at https://aka.ms/dataset-deprecation.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "subscription_id = '381b38e9-9840-4719-a5a0-61d9585e1e91'\n",
    "resource_group_name = 'gasi_rg_centraleuap'\n",
    "workspace_name = \"gasi_ws_centraleuap\"\n",
    "\n",
    "from azureml.core import Workspace as WorkspaceV1\n",
    "ws = WorkspaceV1(workspace_name=workspace_name, resource_group=resource_group_name, subscription_id=subscription_id)\n",
    "\n",
    "ds = Dataset.get(ws, name='beertrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>BeerProduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>3459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1992-02-01</td>\n",
       "      <td>3458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-03-01</td>\n",
       "      <td>4002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1992-04-01</td>\n",
       "      <td>4564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1992-05-01</td>\n",
       "      <td>4221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2011-08-01</td>\n",
       "      <td>10469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2011-09-01</td>\n",
       "      <td>10085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2011-10-01</td>\n",
       "      <td>9612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2011-11-01</td>\n",
       "      <td>10328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>11483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DATE  BeerProduction\n",
       "0   1992-01-01            3459\n",
       "1   1992-02-01            3458\n",
       "2   1992-03-01            4002\n",
       "3   1992-04-01            4564\n",
       "4   1992-05-01            4221\n",
       "..         ...             ...\n",
       "235 2011-08-01           10469\n",
       "236 2011-09-01           10085\n",
       "237 2011-10-01            9612\n",
       "238 2011-11-01           10328\n",
       "239 2011-12-01           11483\n",
       "\n",
       "[240 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AZURE_EXTENSION_DIR=/home/schrodinger/automl/sdk-cli-v2/src/cli/src\n",
      "env: AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED=true\n"
     ]
    }
   ],
   "source": [
    "%env AZURE_EXTENSION_DIR=/home/schrodinger/automl/sdk-cli-v2/src/cli/src\n",
    "%env AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ml\n",
    "from azure.ml import MLClient\n",
    "\n",
    "from azure.core.exceptions import ResourceExistsError\n",
    "\n",
    "from azure.ml.entities import Workspace\n",
    "from azure.ml.entities import AmlCompute\n",
    "from azure.ml.entities import Data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample notebook may use features that are not available in previous versions of the Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was created using version 1.31.0 of the Azure ML SDK\n",
      "You are currently using SDK version 0.0.86 of the Azure ML SDK\n"
     ]
    }
   ],
   "source": [
    "print(\"This notebook was created using version 1.31.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using SDK version\", azure.ml.version.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.ml._ml_client.MLClient at 0x7fbc19e41f90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subscription_id = '381b38e9-9840-4719-a5a0-61d9585e1e91'\n",
    "resource_group_name = 'gasi_rg_centraleuap'\n",
    "workspace_name = \"gasi_ws_centraleuap\"\n",
    "experiment_name = 'automl-classification-text-dnn'\n",
    "\n",
    "client = MLClient(subscription_id, resource_group_name, default_workspace_name=workspace_name)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize MLFlowClient\n",
    "\n",
    "Create an MLFlowClient to interact with the resources that the AutoML job creates, such as models, metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current tracking uri: azureml://master.experiments.azureml-test.net/mlflow/v1.0/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap?\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "########\n",
    "# TODO: The API to get tracking URI is not yet available on Worksapce object.\n",
    "from azureml.core import Workspace as WorkspaceV1\n",
    "ws = WorkspaceV1(workspace_name=workspace_name, resource_group=resource_group_name, subscription_id=subscription_id)\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "########\n",
    "\n",
    "# Not sure why this doesn't work w/o the double + single quotes\n",
    "# mlflow.set_tracking_uri(\"azureml://northeurope.experiments.azureml.net/mlflow/v1.0/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_neu/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_neu?\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a compute cluster\n",
    "This section uses a user-provided compute cluster (named \"dnntext-cluster\" in this example). If a cluster with this name does not exist in the user's workspace, the below code will create a new cluster. You can choose the parameters of the cluster as mentioned in the comments.\n",
    "\n",
    "> Note that if you have an AzureML Data Scientist role, you will not have permission to create compute resources. Talk to your workspace or IT admin to create the compute targets described in this section, if they do not already exist.\n",
    "\n",
    "Whether you provide/select a CPU or GPU cluster, AutoML will choose the appropriate DNN for that setup - BiLSTM or BERT text featurizer will be included in the candidate featurizers on CPU and GPU respectively.  If your goal is to obtain the most accurate model, we recommend you use GPU clusters since BERT featurizers usually outperform BiLSTM featurizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AmlCompute({'name': 'gpu-cluster', 'id': None, 'description': None, 'tags': {}, 'properties': {}, 'base_path': './', 'location': 'centraluseuap', 'type': 'amlcompute', 'enable_public_ip': False, 'resource_id': None, 'provisioning_state': None, 'provisioning_errors': None, 'created_on': None, 'size': 'STANDARD_NC6', 'min_instances': 0, 'max_instances': 2, 'idle_time_before_scale_down': 120, 'identity_type': None, 'user_assigned_identities': None, 'admin_username': 'azureuser', 'admin_password': None, 'ssh_key_value': None, 'vnet_name': None, 'subnet': None, 'priority': None})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set or create compute\n",
    "\n",
    "num_nodes = 2\n",
    "cpu_cluster_name = \"gpu-cluster\"\n",
    "compute = AmlCompute(\n",
    "    name=cpu_cluster_name, size=\"STANDARD_NC6\",\n",
    "    min_instances=0, max_instances=num_nodes,\n",
    "    idle_time_before_scale_down=120\n",
    ")\n",
    "\n",
    "try:\n",
    "    client.compute.create(compute)\n",
    "except ResourceExistsError as re:\n",
    "    print(re)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    \n",
    "    print(\"Could not create compute.\", str(e))\n",
    "    # traceback.print_exc()\n",
    "    # Reload an existing compute target\n",
    "    compute = client.compute.get(cpu_cluster_name)\n",
    "\n",
    "compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "For this notebook we will use 20 Newsgroups data from scikit-learn. We filter the data to contain four classes and take a sample as training data. Please note that for accuracy improvement, more data is needed. For this notebook we provide a small-data example so that you can use this template to use with your larger sized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"text-dnn-data\" # Local directory to store data\n",
    "blobstore_datadir = data_dir # Blob store directory to store data in\n",
    "target_column_name = 'y'\n",
    "feature_column_name = 'X'\n",
    "\n",
    "def get_20newsgroups_data():\n",
    "    '''Fetches 20 Newsgroups data from scikit-learn\n",
    "       Returns them in form of pandas dataframes\n",
    "    '''\n",
    "    remove = ('headers', 'footers', 'quotes')\n",
    "    categories = [\n",
    "        'rec.sport.baseball',\n",
    "        'rec.sport.hockey',\n",
    "        'comp.graphics',\n",
    "        'sci.space',\n",
    "        ]\n",
    "\n",
    "    data = fetch_20newsgroups(subset = 'train', categories = categories,\n",
    "                                    shuffle = True, random_state = 42,\n",
    "                                    remove = remove)\n",
    "    data = pd.DataFrame({feature_column_name: data.data, target_column_name: data.target})\n",
    "\n",
    "    data_train = data[:200]\n",
    "    data_test = data[200:300]    \n",
    "\n",
    "    data_train = remove_blanks_20news(data_train, feature_column_name, target_column_name)\n",
    "    data_test = remove_blanks_20news(data_test, feature_column_name, target_column_name)\n",
    "    \n",
    "    return data_train, data_test\n",
    "    \n",
    "def remove_blanks_20news(data, feature_column_name, target_column_name):\n",
    "    \n",
    "    data[feature_column_name] = data[feature_column_name].replace(r'\\n', ' ', regex=True).apply(lambda x: x.strip())\n",
    "    data = data[data[feature_column_name] != '']\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch data and upload to datastore for use in training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 2 files\n",
      "Uploading text-dnn-data/train_data.csv\n",
      "Uploaded text-dnn-data/train_data.csv, 1 files out of an estimated total of 2\n",
      "Uploading text-dnn-data/test_data.csv\n",
      "Uploaded text-dnn-data/test_data.csv, 2 files out of an estimated total of 2\n",
      "Uploaded 2 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_c9fcbb8e1d6a445bb3487e2156229a7a"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train, data_test = get_20newsgroups_data()\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "    \n",
    "train_data_fname = data_dir + '/train_data.csv'\n",
    "test_data_fname = data_dir + '/test_data.csv'\n",
    "\n",
    "data_train.to_csv(train_data_fname, index=False)\n",
    "data_test.to_csv(test_data_fname, index=False)\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir=data_dir, target_path=blobstore_datadir,\n",
    "                    overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once the data is uploaded to the blob store, please create the tabular dataset manually from the UI\n",
    "\n",
    "The files should already be present under the default workspace storage account, under {{data_dir}} specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data({'is_anonymous': False, 'name': 'textdnn', 'id': '/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap/data/textdnn/versions/1', 'description': None, 'tags': {}, 'properties': {}, 'base_path': './', 'creation_context': <azure.ml._restclient.v2021_03_01_preview.models._models_py3.SystemData object at 0x7f1dc4395750>, 'version': 1, 'datastore': '/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap/datastores/workspaceblobstore', 'path': 'text-dnn-data/train_data.csv', 'local_path': None})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"textdnn\"    # use the name chosen on the UI\n",
    "dataset_version = 1\n",
    "\n",
    "try:\n",
    "    train_dataset = client.data.get(dataset_name, dataset_version)\n",
    "#     training_data = Data(name=dataset_name, version=dataset_version, local_path=\"./data\")\n",
    "#     training_data = client.data.create_or_update(training_data)\n",
    "#     print(\"Uploaded to path  : \", data.path)\n",
    "#     print(\"Datastore location: \", data.datastore)\n",
    "except Exception as e:\n",
    "    print(\"Could not create dataset. \", str(e))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare AutoML run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the blocked_models parameter to exclude some models that can take a longer time to train on some text datasets. You can choose to remove models from the blocked_models list but you may need to increase the experiment_timeout_hours parameter value to get results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 30,\n",
    "    \"primary_metric\": 'accuracy',\n",
    "    \"max_concurrent_iterations\": num_nodes, \n",
    "    \"max_cores_per_iteration\": -1,\n",
    "    \"enable_dnn\": True,\n",
    "    \"enable_early_stopping\": True,\n",
    "    \"validation_size\": 0.3,\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"enable_voting_ensemble\": False,\n",
    "    \"enable_stack_ensemble\": False,\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             compute_target=compute_target,\n",
    "                             training_data=train_dataset,\n",
    "                             label_column_name=target_column_name,\n",
    "                             blocked_models = ['LightGBM', 'XGBoostClassifier'],\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoMLJob({'name': 'f0164207-1d4b-4c23-8e17-44ec43d773d3', 'id': None, 'description': None, 'tags': {}, 'properties': {'save_mlflow': True}, 'base_path': './', 'type': 'automl_job', 'creation_context': None, 'experiment_name': 'classification-text-dnn', 'status': None, 'interaction_endpoints': None, 'log_files': None, 'output': None, 'general_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.GeneralSettings object at 0x7f1dc3f39e10>, 'data_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.DataSettings object at 0x7f1dc3f39f50>, 'limit_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.LimitSettings object at 0x7f1dc3f39fd0>, 'forecasting_settings': None, 'training_settings': <azure.ml.entities._job.automl.training_settings.TrainingSettings object at 0x7f1dc3f39a10>, 'featurization_settings': <azure.ml.entities._job.automl.featurization.FeaturizationSettings object at 0x7f1dc3f39e90>, 'compute': {'instance_count': None, 'target': 'gpu-cluster', 'is_local': False, 'instance_type': None, 'location': None, 'properties': None}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azure.ml._restclient.v2020_09_01_preview.models import (\n",
    "    GeneralSettings,\n",
    "    DataSettings,\n",
    "    LimitSettings,\n",
    "    TrainingDataSettings,\n",
    "    ValidationDataSettings,\n",
    "    TestDataSettings,\n",
    "    FeaturizationSettings,\n",
    ")\n",
    "\n",
    "from azure.ml.entities._job.automl.training_settings import TrainingSettings\n",
    "from azure.ml.entities._job.automl.featurization import FeaturizationSettings\n",
    "from azure.ml.entities import AutoMLJob, ComputeConfiguration\n",
    "\n",
    "\n",
    "compute_settings = ComputeConfiguration(target=compute.name)\n",
    "\n",
    "general_settings = GeneralSettings(\n",
    "    task_type=\"classification\",\n",
    "    primary_metric= \"accuracy\",\n",
    "    log_verbosity=\"Info\")\n",
    "\n",
    "limit_settings = LimitSettings(\n",
    "    timeout=30,\n",
    "    trial_timeout=10,    # without a default trial timeout, job creation results in a validation error\n",
    "    max_concurrent_trials=num_nodes,\n",
    "    enable_early_termination=True)\n",
    "\n",
    "training_data_settings = TrainingDataSettings(\n",
    "    dataset_arm_id=\"{}:{}\".format(train_dataset.name, train_dataset.version)\n",
    ")\n",
    "validation_data_settings = ValidationDataSettings(validation_data_size=0.3)\n",
    "\n",
    "data_settings = DataSettings(\n",
    "    training_data=training_data_settings,\n",
    "    target_column_name=target_column_name,\n",
    "    validation_data=validation_data_settings\n",
    ")\n",
    "\n",
    "featurization_settings = FeaturizationSettings(\n",
    "    featurization_config=\"auto\"\n",
    ")\n",
    "\n",
    "training_settings = TrainingSettings(\n",
    "    block_list_models=['LightGBM', 'XGBoostClassifier'],\n",
    "    enable_dnn_training=True,\n",
    "    enable_vote_ensemble=False,\n",
    "    enable_stack_ensemble=False\n",
    ")\n",
    "\n",
    "extra_automl_settings = {\"save_mlflow\": True}\n",
    "\n",
    "automl_job = AutoMLJob(\n",
    "    compute=compute_settings,\n",
    "    general_settings=general_settings,\n",
    "    limit_settings=limit_settings,\n",
    "    data_settings=data_settings,\n",
    "    training_settings=training_settings,\n",
    "    featurization_settings=featurization_settings,\n",
    "    properties=extra_automl_settings,\n",
    ")\n",
    "\n",
    "automl_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_job.dump(\"./automl_dnn_job.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit AutoML Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoMLJob({'name': 'f0164207-1d4b-4c23-8e17-44ec43d773d3', 'id': '/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap/jobs/f0164207-1d4b-4c23-8e17-44ec43d773d3', 'description': None, 'tags': {}, 'properties': {'save_mlflow': 'True', 'errors': 'Setup iteration failed: {\"run_error\": {\"exception\": {\"message\": \"FitException:\\\\n\\\\tMessage: Encountered an internal AutoML error.                                      Exception raised while initializing PretrainedTextDnn model for text dnn.                                     Please try the experiment again, and contact support if the error persists.                                      Error Message/Code: {error_details}\\\\n\\\\tInnerException: AttributeError\\\\n\\\\tErrorResponse \\\\n{\\\\n    \\\\\"error\\\\\": {\\\\n        \\\\\"code\\\\\": \\\\\"SystemError\\\\\",\\\\n        \\\\\"message\\\\\": \\\\\"Encountered an internal AutoML error.                                      Exception raised while initializing PretrainedTextDnn model for text dnn.                                     Please try the experiment again, and contact support if the error persists.                                      Error Message/Code: {error_details}\\\\\",\\\\n        \\\\\"details_uri\\\\\": \\\\\"https://docs.microsoft.com/azure/machine-learning/resource-known-issues#automated-machine-learning\\\\\",\\\\n        \\\\\"target\\\\\": \\\\\"PretrainedTextDNNTransformer\\\\\",\\\\n        \\\\\"inner_error\\\\\": {\\\\n            \\\\\"code\\\\\": \\\\\"ClientError\\\\\",\\\\n            \\\\\"inner_error\\\\\": {\\\\n                \\\\\"code\\\\\": \\\\\"AutoMLInternal\\\\\"\\\\n            }\\\\n        },\\\\n        \\\\\"reference_code\\\\\": \\\\\"0a6fbcb2-5df3-44cf-b142-385e678acb95\\\\\"\\\\n    }\\\\n}\", \"exception_class\": \"FitException\"}, \"traceback\": \"  File \\\\\"setup_run.py\\\\\", line 95, in execute\\\\n    verifier=verifier\\\\n  File \\\\\"featurization_phase.py\\\\\", line 106, in run\\\\n    feature_sweeped_state_container\\\\n  File \\\\\"_featurization_orchestration.py\\\\\", line 178, in orchestrate_featurization\\\\n    verifier)\\\\n  File \\\\\"_featurization_execution.py\\\\\", line 348, in featurize_train_valid_data\\\\n    feature_sweeping_config=feature_sweeping_config\\\\n  File \\\\\"_featurization_execution.py\\\\\", line 84, in _featurize_training_data\\\\n    experiment_observer=experiment_observer\\\\n  File \\\\\"_featurization_execution.py\\\\\", line 258, in _get_transformer_x\\\\n    x_transform = ml_engine.featurize(x, y, dt)\\\\n  File \\\\\"ml_engine.py\\\\\", line 92, in featurize\\\\n    return transformer.fit_transform(x, y)\\\\n  File \\\\\"base.py\\\\\", line 574, in fit_transform\\\\n    return self.fit(X, y, **fit_params).transform(X)\\\\n  File \\\\\"logging_utilities.py\\\\\", line 305, in debug_log_wrapped\\\\n    r = f(self, *args, **kwargs)\\\\n  File \\\\\"data_transformer.py\\\\\", line 284, in fit\\\\n    self.fit_individual_transformer_mapper(transformer_mapper, df, y)\\\\n  File \\\\\"data_transformer.py\\\\\", line 311, in fit_individual_transformer_mapper\\\\n    transformer_mapper.mapper.fit(df, y)\\\\n  File \\\\\"dataframe_mapper.py\\\\\", line 214, in fit\\\\n    _call_fit(transformers.fit, Xt, y)\\\\n  File \\\\\"pipeline.py\\\\\", line 24, in _call_fit\\\\n    return fit_method(X, y, **kwargs)\\\\n  File \\\\\"pipeline.py\\\\\", line 354, in fit\\\\n    self._final_estimator.fit(Xt, y, **fit_params)\\\\n  File \\\\\"pretrained_text_dnn_transformer.py\\\\\", line 253, in fit\\\\n    self._initialize()\\\\n  File \\\\\"pretrained_text_dnn_transformer.py\\\\\", line 346, in _initialize\\\\n    inner_exception=e) from e\\\\n\", \"has_pii\": false, \"is_critical\": true}}'}, 'base_path': './', 'type': 'automl_job', 'creation_context': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.SystemData object at 0x7f97e680d710>, 'experiment_name': 'classification-text-dnn', 'status': 'Failed', 'interaction_endpoints': {'Tracking': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.JobEndpoint object at 0x7f97ac060250>, 'Studio': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.JobEndpoint object at 0x7f97ac0601d0>}, 'log_files': None, 'output': None, 'general_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.GeneralSettings object at 0x7f97ac060290>, 'data_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.DataSettings object at 0x7f97e6872450>, 'limit_settings': <azure.ml._restclient.v2020_09_01_preview.models._models_py3.LimitSettings object at 0x7f97ac0604d0>, 'forecasting_settings': <azure.ml.entities._job.automl.forecasting.ForecastingSettings object at 0x7f97ac0c4e10>, 'training_settings': <azure.ml.entities._job.automl.training_settings.TrainingSettings object at 0x7f97e68f1a90>, 'featurization_settings': <azure.ml.entities._job.automl.featurization.FeaturizationSettings object at 0x7f97e68f1dd0>, 'compute': {'instance_count': None, 'target': 'gpu-cluster', 'is_local': False, 'instance_type': None, 'location': None, 'properties': None}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "created_job = client.jobs.create_or_update(automl_job)\n",
    "created_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studio URL:  https://ml.azure.com/runs/f0164207-1d4b-4c23-8e17-44ec43d773d3?wsid=/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourcegroups/gasi_rg_centraleuap/workspaces/gasi_ws_centraleuap&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "print(\"Studio URL: \", created_job.interaction_endpoints[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displaying the run objects gives you links to the visual tools in the Azure Portal. Go try them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the Best Model\n",
    "Below we select the best model pipeline from our iterations, use it to test on test data on the same compute cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can test the model locally to get a feel of the input/output. When the model contains BERT, this step will require pytorch and pytorch-transformers installed in your local environment. The exact versions of these packages can be found in the **automl_env.yml** file located in the local copy of your MachineLearningNotebooks folder here:\n",
    "MachineLearningNotebooks/how-to-use-azureml/automated-machine-learning/automl_env.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found best child run id:  AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Run: data=<RunData: metrics={'AUC_macro': 0.9075892857142858,\n",
       " 'AUC_micro': 0.9130003963535473,\n",
       " 'AUC_weighted': 0.904351395730706,\n",
       " 'accuracy': 0.7586206896551724,\n",
       " 'average_precision_score_macro': 0.8230804014367542,\n",
       " 'average_precision_score_micro': 0.8139691450897437,\n",
       " 'average_precision_score_weighted': 0.8161927680357136,\n",
       " 'balanced_accuracy': 0.765625,\n",
       " 'f1_score_macro': 0.7685983397190295,\n",
       " 'f1_score_micro': 0.7586206896551724,\n",
       " 'f1_score_weighted': 0.7601400449200687,\n",
       " 'log_loss': 0.754499859554939,\n",
       " 'matthews_correlation': 0.6800317795420728,\n",
       " 'norm_macro_recall': 0.6875,\n",
       " 'precision_score_macro': 0.781547619047619,\n",
       " 'precision_score_micro': 0.7586206896551724,\n",
       " 'precision_score_weighted': 0.7722906403940887,\n",
       " 'recall_score_macro': 0.765625,\n",
       " 'recall_score_micro': 0.7586206896551724,\n",
       " 'recall_score_weighted': 0.7586206896551724,\n",
       " 'weighted_accuracy': 0.7529411764705882}, params={}, tags={'_aml_system_ComputeTargetStatus': '{\"AllocationState\":\"steady\",\"PreparingNodeCount\":0,\"RunningNodeCount\":1,\"CurrentNodeCount\":2}',\n",
       " '_aml_system_automl_is_child_run_end_telemetry_event_logged': 'True',\n",
       " '_aml_system_azureml.automlComponent': 'AutoML',\n",
       " 'mlflow.parentRunId': 'AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5',\n",
       " 'mlflow.source.name': 'automl_driver.py',\n",
       " 'mlflow.source.type': 'JOB',\n",
       " 'model_explain_run_id': 'AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_ModelExplain',\n",
       " 'model_explanation': 'True'}>, info=<RunInfo: artifact_uri='azureml://experiments/automl-classification-text-dnn/runs/AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_12/artifacts', end_time=1625596673436, experiment_id='0fa5f64a-dde1-43fd-8409-988639922d65', lifecycle_stage='active', run_id='AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_12', run_uuid='AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_12', start_time=1625596601310, status='FINISHED', user_id='d0b038bb-162b-4d49-b8fe-5786e199f6fb'>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# TODO: Use this run, as it has MLFlow model stored on the run\n",
    "job_name = \"AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5\"\n",
    "# job_name = created_job.name\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
    "\n",
    "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
    "print(\"Found best child run id: \", best_child_run_id)\n",
    "\n",
    "best_run_customized = mlflow_client.get_run(best_child_run_id)\n",
    "best_run_customized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model locally requires Torch to be installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.9.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torch-1.9.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (175.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.5 MB 15 kB/s  eta 0:00:011    |████▍                           | 23.9 MB 12.6 MB/s eta 0:00:13\n",
      "\u001b[?25hCollecting torchvision==0.10.0+cpu\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.10.0%2Bcpu-cp37-cp37m-linux_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 12.9 MB/s eta 0:00:01    |███████████                     | 5.4 MB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchaudio==0.9.0\n",
      "  Downloading torchaudio-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from torch==1.9.0+cpu) (3.10.0.0)\n",
      "Collecting pillow>=5.3.0\n",
      "  Downloading Pillow-8.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from torchvision==0.10.0+cpu) (1.18.5)\n",
      "Installing collected packages: torch, pillow, torchvision, torchaudio\n",
      "Successfully installed pillow-8.3.0 torch-1.9.0+cpu torchaudio-0.9.0 torchvision-0.10.0+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-transformers==1.0.0\n",
      "  Downloading pytorch_transformers-1.0.0-py3-none-any.whl (137 kB)\n",
      "\u001b[K     |████████████████████████████████| 137 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from pytorch-transformers==1.0.0) (1.9.0+cpu)\n",
      "Requirement already satisfied: tqdm in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from pytorch-transformers==1.0.0) (4.60.0)\n",
      "Collecting regex\n",
      "  Downloading regex-2021.7.6-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
      "\u001b[K     |████████████████████████████████| 721 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from pytorch-transformers==1.0.0) (2.25.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from pytorch-transformers==1.0.0) (1.18.5)\n",
      "Requirement already satisfied: boto3 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from pytorch-transformers==1.0.0) (1.15.18)\n",
      "Requirement already satisfied: typing-extensions in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from torch>=0.4.1->pytorch-transformers==1.0.0) (3.10.0.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from boto3->pytorch-transformers==1.0.0) (0.3.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from boto3->pytorch-transformers==1.0.0) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.18 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from boto3->pytorch-transformers==1.0.0) (1.18.18)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.18->boto3->pytorch-transformers==1.0.0) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.18->boto3->pytorch-transformers==1.0.0) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.0,>=1.18.18->boto3->pytorch-transformers==1.0.0) (1.16.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from requests->pytorch-transformers==1.0.0) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from requests->pytorch-transformers==1.0.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/schrodinger/anaconda3/envs/dpv2sdk/lib/python3.7/site-packages (from requests->pytorch-transformers==1.0.0) (2.10)\n",
      "Installing collected packages: sentencepiece, regex, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.0.0 regex-2021.7.6 sentencepiece-0.1.96\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-transformers==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "fitted_model = mlflow.sklearn.load_model(\"runs:/{}/outputs\".format(best_run_customized.info.run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see what text transformations are used to convert text data to features for this dataset, including deep learning transformations based on BiLSTM or Transformer (BERT is one implementation of a Transformer) models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StringCast-CharGramTfIdf',\n",
       " 'StringCast-WordGramTfIdf',\n",
       " 'StringCast-StringConcatTransformer-PretrainedTextDNNTransformer']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transformations_used = []\n",
    "for column_group in fitted_model.named_steps['datatransformer'].get_featurization_summary():\n",
    "    text_transformations_used.extend(column_group['Transformations'])\n",
    "text_transformations_used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the best model\n",
    "We now register the best fitted model from the AutoML Run for use in future deployments.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get results stats, extract the best model from AutoML run, download and register the resultant best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This is v1 Run dependencies, won't work on v2\n",
    "\n",
    "# summary_df = get_result_df(automl_run)\n",
    "# best_dnn_run_id = summary_df['run_id'].iloc[0]\n",
    "# best_dnn_run = Run(experiment, best_dnn_run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /tmp/artifact_downloads/AutoML_51844331-1691-4c7f-b65f-4be28f2c68b5_12/outputs/outputs already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "def download_outputs_via_mlflow_client(mlflow_client, run_id, path) -> str:\n",
    "    \"\"\"Download the `path` (file or dir) from the run artifacts, returns the local path download\"\"\"\n",
    "    local_dir = \"/tmp/artifact_downloads/{}\".format(run_id)\n",
    "    local_path = os.path.join(local_dir, path)\n",
    "    if os.path.exists(local_path):\n",
    "        print(\"Directory {} already exists. Skipping download.\".format(os.path.join(local_path, path)))\n",
    "    else:\n",
    "        # download outputs\n",
    "        if not os.path.exists(local_path):\n",
    "            os.makedirs(local_path, exist_ok = False) \n",
    "\n",
    "        local_path = mlflow_client.download_artifacts(run_id, path, local_path)\n",
    "        print(\"Artifacts downloaded to: {}\".format(local_path))\n",
    "        print(\"Artifacts: {}\".format(os.listdir(local_path)))\n",
    "    return local_path\n",
    "\n",
    "output_path = download_outputs_via_mlflow_client(mlflow_client, best_run_customized.info.run_id, \"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in your Azure Machine Learning Workspace. If you previously registered a model, please make sure to delete it so as to replace it with this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading model.pkl: 126MB [00:42, 2.99MB/s]                            \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model({'is_anonymous': False, 'name': 'textDNN-20News', 'id': '/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap/models/textDNN-20News/versions/1', 'description': None, 'tags': {}, 'properties': {'azureml.modelFormat': 'CUSTOM'}, 'base_path': './', 'creation_context': <azure.ml._restclient.v2021_03_01_preview.models._models_py3.SystemData object at 0x7fbbb695ad50>, 'version': 1, 'datastore': '/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_centraleuap/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_centraleuap/datastores/workspaceblobstore', 'path': 'LocalUpload/d1bf7479845ea42773f0dcc4588ee132/model.pkl', 'local_path': None, 'utc_time_created': None, 'flavors': {}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register the model\n",
    "from azure.ml.entities._assets import Model\n",
    "\n",
    "model_name = 'textDNN-20News'\n",
    "\n",
    "# Note: This is not using MLFlow's deployment mechanism at all (flavors, scoring script / examples etc.)\n",
    "# Create / register the model\n",
    "# TODO: This doesn't track the lineage (run id) from which the model is created. \n",
    "azure_model = Model(name=model_name, version=1, local_path=os.path.join(output_path, \"model.pkl\"))\n",
    "azure_model = client.models.create_or_update(azure_model)\n",
    "azure_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the best fitted model from the AutoML Run to make predictions on the test set.  \n",
    "\n",
    "Test set schema should match that of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DFW was designed with the STS in mind (which r...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johnny Mize had six three-HR games, which is t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actually I admired the spirit of the fan at th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I don't know a whole lot on Proton, but given ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPECIFIC: Basically to be able to do the thing...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X  y\n",
       "0  DFW was designed with the STS in mind (which r...  3\n",
       "1  Johnny Mize had six three-HR games, which is t...  1\n",
       "2  Actually I admired the spirit of the fan at th...  1\n",
       "3  I don't know a whole lot on Proton, but given ...  3\n",
       "4  SPECIFIC: Basically to be able to do the thing...  3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"./text-dnn-data/test_data.csv\")\n",
    "\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inference run / job is currently not possible\n",
    "\n",
    "# test_experiment = Experiment(ws, experiment_name + \"_test\")\n",
    "\n",
    "# script_folder = os.path.join(os.getcwd(), 'inference')\n",
    "# os.makedirs(script_folder, exist_ok=True)\n",
    "# shutil.copy('infer.py', script_folder)\n",
    "\n",
    "# test_run = run_inference(test_experiment, compute_target, script_folder, best_dnn_run,\n",
    "#                          test_dataset, target_column_name, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display computed metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RunDetails(test_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(test_run.get_metrics())"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "anshirga"
   }
  ],
  "compute": [
   "AML Compute"
  ],
  "datasets": [
   "None"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "None"
  ],
  "friendly_name": "DNN Text Featurization",
  "index_order": 2,
  "kernelspec": {
   "display_name": "Python [conda env:dpv2sdk] *",
   "language": "python",
   "name": "conda-env-dpv2sdk-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "tags": [
   "None"
  ],
  "task": "Text featurization using DNNs for classification"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
