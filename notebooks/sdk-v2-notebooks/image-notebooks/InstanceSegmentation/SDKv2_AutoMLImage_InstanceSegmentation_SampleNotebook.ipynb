{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# Training an Instance Segmentation model using AutoML\n",
    "In this notebook, we go over how you can use AutoML for training an Instance Segmentation model. We will use a small dataset to train the model, demonstrate how you can tune hyperparameters of the model to optimize model performance and deploy the model to use in inference scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To use this notebook, you will need to install the private preview package for AutoML for images from the private index.\n",
    "\n",
    "### Note: Only Python 3.6 and 3.7 are supported for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AZURE_EXTENSION_DIR=/home/schrodinger/automl/sdk-cli-v2/src/cli/src\n",
    "%env AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade \"azureml-train-core<0.1.1\" \"azureml-train-automl<0.1.1\" \"azureml-contrib-dataset<0.1.1\" --extra-index-url \"https://azuremlsdktestpypi.azureedge.net/automl_for_images_private_preview/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: You might need to restart the notebook kernel after installing the packages from within the notebook with the line above, for the packages to be available to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Licensing Information - \n",
    "This preview software is made available to you on the condition that you agree to\n",
    "[your agreement][1] governing your use of Azure, and to the Supplemental Terms of Use for Microsoft Azure Previews[2], which supplement your agreement governing your use of Azure.\n",
    "If you do not have an existing agreement governing your use of Azure, you agree that \n",
    "your agreement governing use of Azure is the [Microsoft Online Subscription Agreement][3]\n",
    "(which incorporates the [Online Services Terms][4]).\n",
    "By using the software you agree to these terms. This software may collect data\n",
    "that is transmitted to Microsoft. Please see the [Microsoft Privacy Statement][5]\n",
    "to learn more about how Microsoft processes personal data.\n",
    "\n",
    "[1]: https://azure.microsoft.com/en-us/support/legal/\n",
    "[2]: https://azure.microsoft.com/en-us/support/legal/preview-supplemental-terms/\n",
    "[3]: https://azure.microsoft.com/en-us/support/legal/subscription-agreement/\n",
    "[4]: http://www.microsoftvolumelicensing.com/DocumentSearch.aspx?Mode=3&DocumentTypeId=46\n",
    "[5]: http://go.microsoft.com/fwlink/?LinkId=248681 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup\n",
    "In order to train and deploy models in Azure ML, you will first need to set up a workspace.\n",
    "\n",
    "An [Azure ML Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#workspace) is an Azure resource that organizes and coordinates the actions of many other Azure resources to assist in executing and sharing machine learning workflows. In particular, an Azure ML Workspace coordinates storage, databases, and compute resources providing added functionality for machine learning experimentation, deployment, inference, and the monitoring of deployed models.\n",
    "\n",
    "Create an Azure ML Workspace within your Azure subscription, or load an existing workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize ML Client\n",
    "Create an MLClient object to interact with AzureML resources such as computes and jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.ml\n",
    "from azure.ml import MLClient\n",
    "from azure.ml.entities import Workspace\n",
    "\n",
    "\n",
    "subscription_id = 'e9b2ec51-5c94-4fa8-809a-dc1e695e4896'\n",
    "resource_group_name = 'sajin_master'\n",
    "workspace_name = \"sajin_enterprise\"\n",
    "\n",
    "# TODO: will we support workspace creation through the nb?\n",
    "client = MLClient(subscription_id, resource_group_name, default_workspace_name=workspace_name)\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialize MLFlow Client\n",
    "Create an MLFlowClient to interact with the resources that the AutoML job creates, such as models, metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-core azureml-mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "experiment_name = \"automl-image-instance-segmentation\"\n",
    "\n",
    "########\n",
    "# TODO: The API to get tracking URI is not yet available on Worksapce object.\n",
    "# from azureml.core import Workspace as WorkspaceV1\n",
    "# ws = WorkspaceV1(workspace_name=workspace_name, resource_group=resource_group_name, subscription_id=subscription_id)\n",
    "# mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "# del ws\n",
    "########\n",
    "\n",
    "# Not sure why this doesn't work w/o the double + single quotes\n",
    "# mlflow.set_tracking_uri(\"azureml://northeurope.experiments.azureml.net/mlflow/v1.0/subscriptions/381b38e9-9840-4719-a5a0-61d9585e1e91/resourceGroups/gasi_rg_neu/providers/Microsoft.MachineLearningServices/workspaces/gasi_ws_neu?\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute target setup\n",
    "You will need to provide a [Compute Target](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#compute-target) that will be used for your AutoML model training. AutoML models for image tasks require GPU SKUs and support NC and ND families. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import AmlCompute\n",
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "\n",
    "cluster_name = \"gpu-cluster\"\n",
    "compute = AmlCompute(name=cluster_name,\n",
    "                     size=\"Standard_NC6\",\n",
    "                     min_instances=0,\n",
    "                     max_instances=4,\n",
    "                     idle_time_before_scale_down=1800)\n",
    "\n",
    "# Load directly from YAML file\n",
    "# compute = Compute.load(\"./compute.yaml\")\n",
    "\n",
    "try:\n",
    "    compute = client.compute.get(cluster_name)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ResourceNotFoundError as re:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    compute = client.compute.create(compute)\n",
    "\n",
    "compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset with input Training Data\n",
    "In order to generate models for computer vision, you will need to bring in labeled image data as input for model training in the form of an AzureML Labeled Dataset. You can either use a Labeled Dataset that you have exported from a Data Labeling project, or create a new Labeled Dataset with your labeled training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use a toy dataset called Fridge Objects, which includes 128 images of 4 classes of beverage container {can, carton, milk bottle, water bottle} photos taken on different backgrounds. We first download and unzip the data locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# download data\n",
    "download_url = 'https://cvbp-secondary.z19.web.core.windows.net/datasets/object_detection/odFridgeObjectsMask.zip'\n",
    "data_file = './odFridgeObjectsMask.zip'\n",
    "urllib.request.urlretrieve(download_url, filename=data_file)\n",
    "\n",
    "# extract files\n",
    "with ZipFile(data_file, 'r') as zip:\n",
    "    print('extracting files...')\n",
    "    zip.extractall()\n",
    "    print('done')\n",
    "    \n",
    "# delete zip file\n",
    "os.remove(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample image from this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename='./odFridgeObjectsMask/images/31.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the downloaded data to JSONL\n",
    "In this example, the fridge object dataset is annotated in Pascal VOC format, where each image corresponds to an xml file. Each xml file contains information on where its corresponding image file is located and also contains information about the bounding boxes and the object labels. In order to use this data to create an AzureML Datset, we first need to convert it to the required JSONL format.\n",
    "\n",
    "The following script is creating two .jsonl files (one for training and one for validation) in the parent folder of the dataset. The train / validation ratio corresponds to 20% of the data going into the validation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The jsonl_converter below relies on numpy, pillow, scikit-image and simplification.\n",
    "# If you haven't have them installed, install them before converting data by runing this cell.\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy==1.18.5 pillow==8.0.1 scikit-image==0.17.2 simplification==0.5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdkv2_jsonl_converter import convert_mask_in_VOC_to_jsonl\n",
    "\n",
    "data_path = \"./odFridgeObjectsMask/\"\n",
    "\n",
    "# Retrieving default datastore that got automatically created when we setup a workspace\n",
    "ds = client.datastores.get_default()\n",
    "\n",
    "###########################################################\n",
    "## Updated to use default ds instead of ws object\n",
    "convert_mask_in_VOC_to_jsonl(data_path, ds)\n",
    "#############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the JSONL file and images to Datastore  \n",
    "In order to use the data for training in Azure ML, we upload it to our Azure ML Workspace via a [Datastore](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets-and-datastores). The datastore provides a mechanism for you to upload/download data, and interact with it from your remote compute targets. It is an abstraction over Azure Storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: AzureBlobDatastore object has no attribute 'upload'\n",
    "# ds.upload(src_dir='./odFridgeObjectsMask', target_path='odFridgeObjectsMask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to create an Azure ML [Dataset](https://docs.microsoft.com/en-us/azure/machine-learning/concept-azure-machine-learning-architecture#datasets-and-datastores) from the data we uploaded to the Datastore. We create one dataset for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset creation (+ from jsonl) not supported yet in v2, and tabular DS is not supported in images v1)\n",
    "from azure.ml.entities import Data\n",
    "\n",
    "# TODO: This doesnt' work, ensure dataset is created via the UI\n",
    "\n",
    "training_dataset_name = \"odFridgeObjectsMaskTrainingDataset\"\n",
    "dataset_version = 1\n",
    "\n",
    "try:\n",
    "#     training_data = client.data.get(training_dataset_name, dataset_version)\n",
    "    training_data = Data(name=training_dataset_name, version=dataset_version, local_path=data_path)\n",
    "    training_data = client.data.create_or_update(training_data)\n",
    "    print(\"Uploaded to path  : \", training_data.path)\n",
    "    print(\"Datastore location: \", training_data.datastore)\n",
    "except Exception as e:\n",
    "    print(\"Could not create dataset. \", str(e))\n",
    "\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation dataset\n",
    "validation_dataset_name = \"odFridgeObjectsMaskValidationDataset\"\n",
    "validation_data = client.data.get(validation_dataset_name, dataset_version)\n",
    "\n",
    "# validation_data = Data(name=validation_dataset_name, version=dataset_version, local_path=data_path)\n",
    "# validation_data = client.data.create_or_update(validation_data)\n",
    "    \n",
    "validation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training dataset name: \" + training_data.name)\n",
    "print(\"Validation dataset name: \" + validation_data.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation dataset is optional. If no validation dataset is specified, by default 20% of your training data will be used for validation. You can control the percentage using the `split_ratio` argument - please refer to the documentation for more details. \n",
    "\n",
    "This is what the training dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: to_pandas_dataframe() not supported yet\n",
    "training_data.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring your AutoML run for image tasks\n",
    "AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm to be used, specify hyperparameter values for your model as well as perform a sweep across the hyperparameter space to generate an optimal model. Parameters for configuring your AutoML runs for image related tasks are specified using the 'AutoMLImageConfig' - please refer to the [documentation](https://github.com/swatig007/automlForImages/blob/main/README.md) for the details on the parameters that can be used and their values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using AutoML for image tasks, you need to specify the model algorithms using the `model_name` parameter. You can either specify the specifc model algorithm/hyperparameter config to use or choose to sweep over the hyperparameter space. Currently supported algorithms for Instance segmentation include 'maskrcnn_resnet50_fpn'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using default hyperparameter values for the specified algorithm\n",
    "Before doing a large sweep to search for the optimal models and hyperparameters, we recommend trying the default values to get a first baseline. Next, you can explore multiple hyperparameters for the same model before sweeping over multiple models and their parameters. This is for employing a more iterative approach, because with multiple models and multiple hyperparameters for each (as we showcase in the next section), the search space grows exponentially and you need more iterations to find optimal configurations.\n",
    "\n",
    "If you wish to use the default hyperparameter values for a given algorithm (say maskrcnn), you can specify the config for your AutoML Image runs as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml._restclient.v2020_09_01_preview.models import (\n",
    "    GeneralSettings,\n",
    "    DataSettings,\n",
    "    TrainingDataSettings,\n",
    "    ValidationDataSettings,\n",
    "    LimitSettings\n",
    "#     TestDataSettings,\n",
    "#     FeaturizationSettings,\n",
    ")\n",
    "\n",
    "from azure.ml.entities._job.automl.training_settings import TrainingSettings\n",
    "from azure.ml.entities._job.automl.featurization import FeaturizationSettings\n",
    "# from azure.ml.entities_job.sweep.search_space import SweepDistribution\n",
    "from azure.ml.entities import AutoMLJob, ComputeConfiguration, Choice\n",
    "\n",
    "\n",
    "compute_settings = ComputeConfiguration(target=cluster_name)\n",
    "\n",
    "general_settings = GeneralSettings(task_type='image-instance-segmentation')\n",
    "\n",
    "training_data_settings = TrainingDataSettings(\n",
    "    dataset_arm_id='{}:{}'.format(training_data.name, training_data.version)\n",
    ")\n",
    "validation_data_settings = ValidationDataSettings(\n",
    "    dataset_arm_id='{}:{}'.format(validation_data.name, validation_data.version),\n",
    ")\n",
    "\n",
    "\n",
    "# TODO: Dont know what this will look like yet in v2 for us\n",
    "# Target column name should not be required for Images\n",
    "data_settings = DataSettings(\n",
    "    training_data=training_data_settings,\n",
    "    target_column_name=\"y\",\n",
    "    validation_data=validation_data_settings\n",
    ")\n",
    "\n",
    "# featurization_settings = FeaturizationSettings(\n",
    "#     featurization_config=\"off\"\n",
    "# )\n",
    "\n",
    "\n",
    "# limit_settings = LimitSettings(\n",
    "#     timeout=60,\n",
    "#     trial_timeout=5,\n",
    "#     max_concurrent_trials=4,\n",
    "#     enable_early_termination=True)\n",
    "\n",
    "# training_settings = TrainingSettings(\n",
    "#     block_list_models=[],\n",
    "#     enable_onnx_compatible_models=True,\n",
    ")\n",
    "\n",
    "######################\n",
    "# TODO: create hyperparameter settings so that this will work\n",
    "\n",
    "search_space = {'model_name': Choice(['maskrcnn_resnet50_fpn'])}\n",
    "\n",
    "hyperparameter_settings = HyperparameterSettings(algorithm='Grid',\n",
    "                                                 search_space=search_space)\n",
    "\n",
    "# TODO: create image settings so that this will work\n",
    "# No img settings in this nb\n",
    "# image_settings = ImageSettings()\n",
    "\n",
    "######################\n",
    "\n",
    "extra_automl_settings = {\"save_mlflow\": True}\n",
    "\n",
    "image_maskrcnn_job = AutoMLJob(compute=compute_settings,\n",
    "                               general_settings=general_settings,\n",
    "                               data_settings=data_settings,\n",
    "                               hyperparameter_settings=hyperparameter_settings,\n",
    "#                                image_settings=image_settings,\n",
    "                               properties=extra_automl_settings)\n",
    "\n",
    "image_maskrcnn_job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting an AutoML run for Computer Vision tasks \n",
    "Once you've created the config settings for your run, you can submit an AutoML run using the config in order to train a vision model using your training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_maskrcnn_job = client.jobs.create_or_update(image_maskrcnn_job)\n",
    "created_maskrcnn_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this for?\n",
    "print(\"Studio URL: \", created_maskrcnn_job.interaction_endpoints[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this for?\n",
    "client.jobs.stream(\"run id goes here?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wait for the remote run to complete (same as automl)\n",
    "# automl_image_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter sweeping for your AutoML models for computer vision tasks\n",
    "In this example, we use the AutoMLImageConfig to train an Instance Segmentation model using maskrcnn_resnet50_fpn which is pretrained on COCO, a large-scale object detection, segmentation, and captioning dataset that contains over 200K labeled images with over 80 label cateogories.\n",
    "\n",
    "When using AutoML for computer vision, you can perform a hyperparameter sweep over a defined parameter space, to find the optimal model. In this example, we sweep over the hyperparameters for each algorithm, choosing from a range of values for learning_rate, optimizer, etc, to generate a model with the optimal 'accuracy'. If hyperparameter values are not specified, then default values are used for the specified algorithm.  \n",
    "\n",
    "We use Random Sampling to pick samples from this parameter space and try a total of 20 iterations with these different samples, running 4 iterations at a time on our compute target, which has been previously set up using 4 nodes.  Please note that the more parameters the space has, the more iterations you need to find optimal models. \n",
    "\n",
    "We also leverage the Bandit early termination policy that terminates poor performing configs (those that are not within 20% slack of the best perfroming config), thus significantly saving compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml._restclient.v2020_09_01_preview.models import (\n",
    "    GeneralSettings,\n",
    "    DataSettings,\n",
    "    LimitSettings,\n",
    "    TrainingDataSettings,\n",
    "    ValidationDataSettings,\n",
    "    TestDataSettings,\n",
    "    FeaturizationSettings,\n",
    ")\n",
    "\n",
    "from azure.ml.entities._job.automl.training_settings import TrainingSettings\n",
    "from azure.ml.entities._job.automl.featurization import FeaturizationSettings\n",
    "# from azure.ml.entities_job.sweep.search_space import SweepDistribution\n",
    "from azure.ml.entities import AutoMLJob, ComputeConfiguration, Choice, Uniform\n",
    "\n",
    "from azure.ml._restclient.v2021_03_01_preview.models import BanditPolicy\n",
    "\n",
    "\n",
    "compute_settings = ComputeConfiguration(target=cluster_name)\n",
    "\n",
    "general_settings = GeneralSettings(task_type=\"image-instance-segmentation\",\n",
    "                                   primary_metric=\"mean_average_precision\")\n",
    "\n",
    "limit_settings = LimitSettings(max_total_trials=20,\n",
    "                               max_concurrent_trials=4)\n",
    "\n",
    "\n",
    "# TODO: Dont know what this will look like yet in v2 for us\n",
    "# Target column name should not be required for Images\n",
    "data_settings = DataSettings(\n",
    "    training_data=training_data_settings,\n",
    "    target_column_name=\"\",\n",
    "    validation_data=validation_data_settings)\n",
    "\n",
    "######################\n",
    "# TODO: create HP settings so that this will work\n",
    "\n",
    "serach_space = { \n",
    "    'model_name': Choice(['maskrcnn_resnet50_fpn']),\n",
    "    'learning_rate': Uniform(min_value=0.0001, max_value=0.001),\n",
    "    #'warmup_cosine_lr_warmup_epochs': Choice([0, 3]),\n",
    "    'optimizer': Choice(['sgd', 'adam', 'adamw']),\n",
    "    'min_size': Choice([600, 800])\n",
    "}\n",
    "bandit_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.2, delay_evaluation=6)                       \n",
    "\n",
    "hyperparameter_settings = HyperparameterSettings(algorithm='Random',\n",
    "                                                 search_space=search_space,\n",
    "                                                 early_termination=bandit_policy)\n",
    "    \n",
    "# TODO: add image settings (none in this notebook though)\n",
    "# image_settings = ImageSettings()\n",
    "\n",
    "######################\n",
    "\n",
    "extra_automl_settings = {\"save_mlflow\": True}\n",
    "\n",
    "image_job = AutoMLJob(compute=compute_settings,\n",
    "                      general_settings=general_settings,\n",
    "                      limit_settings=limit_settings,\n",
    "                      data_settings=data_settings,\n",
    "                      hyperparameter_settings=hyperparameter_settings,\n",
    "#                       image_settings=image_settings,\n",
    "                      properties=extra_automl_settings)\n",
    "\n",
    "image_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_image_job = client.jobs.create_or_update(image_job)\n",
    "created_image_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whats this?\n",
    "print(\"Studio URL: \", created_image_job.interaction_endpoints[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is this for?\n",
    "client.jobs.stream(\"run id goes here?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wait for the remote run to complete (same as automl)\n",
    "# automl_image_run.wait_for_completion(wait_post_processing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the optimal vision model from the AutoML run (We need MLFlow for this (?))\n",
    "Once the run completes, we can register the model that was created from the best run (configuration that resulted in the best primary metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 FOR TESTING:\n",
    "import azureml.core\n",
    "from azureml.core import Model\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)\n",
    "training_run_id =\"AutoML_5e3d7846-56df-46ec-b6d6-00244297cdb1_HD_0\"\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "best_child_run = Run(experiment, training_run_id)\n",
    "\n",
    "# Get the name of the model\n",
    "run_properties = best_child_run.get_properties()\n",
    "\n",
    "run_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From AutoML notebook : Can't access properties right now - https://msdata.visualstudio.com/Vienna/_workitems/edit/1252056\n",
    "# Will probably need MlFlow support to do this, which is NOT yet implemented for Images\n",
    "model_name = \"AutoML5e3d784650\" # best_run.properties['model_name']\n",
    "output_path= \"outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Uses ML Flow to get parent/child run###########################################################\n",
    "# Mlflow does not work with images\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "job_name = \"AutoML_5e3d7846-56df-46ec-b6d6-00244297cdb1\"\n",
    "# job_name = created_job.name\n",
    "\n",
    "mlflow_client = MlflowClient()\n",
    "\n",
    "# TODO: gives: MlflowException: Run '<RunId>' not found (got same error in bank notebook)\n",
    "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
    "\n",
    "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
    "print(\"Found best child run id: \", best_child_run_id)\n",
    "\n",
    "best_child_run = mlflow_client.get_run(best_child_run_id)\n",
    "best_child_run\n",
    "#######################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities._assets import Model\n",
    "\n",
    "# Note: This is not using MLFlow's deployment mechanism at all (flavors, scoring script / examples etc.)\n",
    "# Create / register the model\n",
    "# TODO: This doesn't track the lineage (run id) from which the model is created. \n",
    "\n",
    "# TODO: Remote path not supported yet, something called 'ModelFormat' is going to be supported in DPv2\n",
    "# That'll take in the location of MLFlow model directory, and take it from there for deployment\n",
    "# for now, model needs to be locally downloaded in order to deploy\n",
    "azure_model = Model(name=model_name, version=1, local_path=os.path.join(output_path, \"model.pt\"))\n",
    "\n",
    "azure_model = client.models.create_or_update(azure_model)\n",
    "azure_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Register via. MLFlow (Not recommended)\n",
    "# AttributeError: 'Run' object has no attribute 'info'\n",
    "\n",
    "child_run_id = job_name\n",
    "mlflow_model = mlflow.register_model(\n",
    "    \"runs:/{}/outputs\".format(child_run_id),\n",
    "    \"mlflow_automl_model\"\n",
    ")\n",
    "\n",
    "mlflow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from v1\n",
    "model_name = run_properties['model_name']\n",
    "azure_model = best_child_run.register_model(model_name=model_name, model_path='outputs/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model as a web service\n",
    "Once you have your trained model, you can deploy the model on Azure. You can deploy your trained model as a web service on Azure Container Instances ([ACI](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-container-instance)) or Azure Kubernetes Service ([AKS](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-azure-kubernetes-service)). ACI is the perfect option for testing deployments, while AKS is better suited for for high-scale, production usage.  \n",
    "In this tutorial, we will deploy the model as a web service in AKS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then deploy the model as an AKS web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ml.entities import Endpoint, ManagedOnlineEndpoint, Environment, \\\n",
    "CodeConfiguration, ManagedOnlineDeployment, ManualScaleSettings, Code\n",
    "\n",
    "# conda_environment_yaml = best_child_run.properties['conda_env_data_location']\n",
    "# inference_script_file_name = best_child_run.properties['scoring_data_location']\n",
    "\n",
    "inference_script_file_name = os.path.join(output_path, \"scoring_file_v_1_0_0.py\")\n",
    "conda_environment_yaml = os.path.join(output_path, \"conda_env_v_1_0_0.yml\")\n",
    "\n",
    "print(\"Inference File: \", inference_script_file_name)\n",
    "\n",
    "# Figure out if can just use training run env\n",
    "# Ex: environment = best_child_run.get_environment()\n",
    "print(\"Conda Environment File: \", conda_environment_yaml)\n",
    "\n",
    "assert os.path.exists(inference_script_file_name)\n",
    "assert os.path.exists(conda_environment_yaml)\n",
    "\n",
    "id_suffix = job_name[-6:]\n",
    "print(id_suffix)\n",
    "\n",
    "# Prepare the deployment configuration\n",
    "\n",
    "# Reuse training env environment\n",
    "env_name = best_child_run.runDefintion['name']\n",
    "env_version = best_child_run.runDefintion['version']\n",
    "environment = client.environments.get(name=env_name, version=env_version)\n",
    "\n",
    "# Load from conda env yml\n",
    "# environment = Environment(\n",
    "#     name=\"environment-{}\".format(id_suffix),\n",
    "#     version=1,\n",
    "#     path=\".\",\n",
    "#     conda_file=conda_environment_yaml,\n",
    "#     docker_image=\"mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210301.v1\",# whats this\n",
    "# )\n",
    "\n",
    "code = Code(\n",
    "    name=\"environment-{}\".format(id_suffix),\n",
    "    version=1,\n",
    "    local_path=inference_script_file_name,\n",
    ")\n",
    "code_configuration = CodeConfiguration(\n",
    "    code=code,\n",
    "    scoring_script=inference_script_file_name\n",
    ")\n",
    "\n",
    "scale_settings = ManualScaleSettings(\n",
    "    scale_type=\"Manual\",\n",
    "    min_instances=1,\n",
    "    max_instances=2,\n",
    "    instance_count=1\n",
    ")\n",
    "\n",
    "# Valid values are: [Standard_D16s_v3, Standard_D2s_v3, Standard_D32s_v3, Standard_D4s_v3, Standard_D64s_v3,\n",
    "# Standard_D8s_v3, Standard_DS2_v2, Standard_DS3_v2, Standard_DS4_v2, Standard_DS5_v2, Standard_F16s_v2,\n",
    "# Standard_F2s_v2, Standard_F32s_v2, Standard_F48s_v2, Standard_F4s_v2, Standard_F64s_v2, Standard_F72s_v2,\n",
    "# Standard_F8s_v2, Standard_NC12s_v2, Standard_NC12s_v3, Standard_NC16as_T4_v3, Standard_NC24s_v2, Standard_NC24s_v3, \n",
    "# Standard_NC4as_T4_v3, Standard_NC64as_T4_v3, Standard_NC6s_v2, Standard_NC6s_v3, Standard_NC8as_T4_v3, Standard_ND24s,\n",
    "# Standard_ND40rs_v2, Standard_ND96asr_v4]\n",
    "\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=\"deployment-{}\".format(id_suffix),\n",
    "    model=azure_model,\n",
    "    environment=environment,\n",
    "    code_configuration=code_configuration,\n",
    "    instance_type=\"Standard_NC6s_v2\",\n",
    "    scale_settings=scale_settings,\n",
    "                                    )\n",
    "online_endpoint = ManagedOnlineEndpoint(\n",
    "    name=\"endpoint-{}\".format(id_suffix),\n",
    "    deployments=[deployment],\n",
    "    description=\"Demo Image model deployment\",\n",
    "    tags={\"deployed_using\": \"sdkv2\"}\n",
    ")\n",
    "##### Loading from YAML\n",
    "# endpoint = Endpoint.load(\"/home/schrodinger/automl/Easy-AutoML-MLOps/notebooks/3-automl-remote-compute-run/endpoint.yml\")\n",
    "\n",
    "try:\n",
    "    client.endpoints.create(online_endpoint)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"Deployment failed: \", str(e))\n",
    "    traceback.print_exc()\n",
    "    \n",
    "#################TODO###################################\n",
    "# Fails with quota issue, but on portal shows as succeded\n",
    "# https://ml.azure.com/endpoints/realtime/endpoint-2538a3/detail?wsid=/subscriptions/e9b2ec51-5c94-4fa8-809a-dc1e695e4896/resourcegroups/sajin_master/workspaces/sajin_enterprise&tid=72f988bf-86f1-41af-91ab-2d7cd011db47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logs from deployed web service\n",
    "# Returns nothing at the moment (?)\n",
    "client.endpoints.get_deployment_logs(online_endpoint.name, deployment.name, lines=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the web service\n",
    "Finally, let's test our deployed web service to predict new images. You can pass in any image. In this case, we'll use a random image from the dataset and pass it to te scoring URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the web service - No scoring_uri even though endpoint succeeded?\n",
    "scoring_uri = online_endpoint.scoring_uri\n",
    "scoring_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL for the web service\n",
    "scoring_uri = online_endpoint.scoring_uri\n",
    "\n",
    "# TODO: no aks_service anymore to get keys\n",
    "# If the service is authenticated, set the key or token\n",
    "key, _ = aks_service.get_keys()\n",
    "\n",
    "sample_image = './test_image.jpg'\n",
    "\n",
    "# Load image data\n",
    "data = open(sample_image, 'rb').read()\n",
    "\n",
    "# Set the content type\n",
    "headers = {'Content-Type': 'application/octet-stream'}\n",
    "\n",
    "# If authentication is enabled, set the authorization header\n",
    "headers['Authorization'] = f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, data, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions\n",
    "Now that we have scored a test image, we can visualize the predictions for this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.lines import Line2D\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "IMAGE_SIZE = (18,12)\n",
    "plt.figure(figsize=IMAGE_SIZE)\n",
    "img_np=mpimg.imread(sample_image)\n",
    "img = Image.fromarray(img_np.astype('uint8'),'RGB')\n",
    "x, y = img.size\n",
    "\n",
    "fig,ax = plt.subplots(1, figsize=(15,15))\n",
    "# Display the image\n",
    "ax.imshow(img_np)\n",
    "\n",
    "# draw box and label for each detection \n",
    "detections = json.loads(resp.text)\n",
    "for detect in detections['boxes']:\n",
    "    label = detect['label']\n",
    "    box = detect['box']\n",
    "    polygon = detect['polygon']\n",
    "    conf_score = detect['score']\n",
    "    if conf_score > 0.6:\n",
    "        ymin, xmin, ymax, xmax =  box['topY'],box['topX'], box['bottomY'],box['bottomX']\n",
    "        topleft_x, topleft_y = x * xmin, y * ymin\n",
    "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
    "        print('{}: [{}, {}, {}, {}], {}'.format(detect['label'], round(topleft_x, 3), \n",
    "                                                round(topleft_y, 3), round(width, 3), \n",
    "                                                round(height, 3), round(conf_score, 3)))\n",
    "\n",
    "        color = np.random.rand(3) #'red'\n",
    "        rect = patches.Rectangle((topleft_x, topleft_y), width, height, \n",
    "                                 linewidth=2, edgecolor=color,facecolor='none')\n",
    "\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
    "        \n",
    "        polygon_np = np.array(polygon[0])\n",
    "        polygon_np = polygon_np.reshape(-1, 2)\n",
    "        polygon_np[:, 0] *= x\n",
    "        polygon_np[:, 1] *= y\n",
    "        poly = patches.Polygon(polygon_np, True, facecolor=color, alpha=0.4)\n",
    "        ax.add_patch(poly)\n",
    "        poly_line = Line2D(polygon_np[:, 0], polygon_np[:, 1], linewidth=2,\n",
    "                           marker='o', markersize=8, markerfacecolor=color)\n",
    "        ax.add_line(poly_line)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dpv2)",
   "language": "python",
   "name": "dpv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
