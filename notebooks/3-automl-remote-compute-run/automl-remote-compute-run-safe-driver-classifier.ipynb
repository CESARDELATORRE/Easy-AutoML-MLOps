{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML on remote AML Compute (Porto Seguro's Safe Driving Prediction)\n",
    "\n",
    "This notebook is refactored (from the original AutoML local training notebook) to use AutoML on remote AML compute, in a cluster.\n",
    "It also uses AML Datasets for training instead of Pandas Dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Needed Packages\n",
    "\n",
    "Import the packages needed for this solution notebook. The most widely used package for machine learning is [scikit-learn](https://scikit-learn.org/stable/), [pandas](https://pandas.pydata.org/docs/getting_started/index.html#getting-started), and [numpy](https://numpy.org/). These packages have various features, as well as a lot of clustering, regression and classification algorithms that make it a good choice for data mining and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Azure ML SDK version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"This notebook was created and tested using version 1.2.0 of the Azure ML SDK\")\n",
    "print(\"You are currently using version\", azureml.core.VERSION, \"of the Azure ML SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Get Azure ML Workspace to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "# Get Workspace defined in by default config.json file\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Submit dataset file into DataStore (Azure Blob under the covers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()\n",
    "datastore.upload(src_dir='../../data/', \n",
    "                 target_path='Datasets/porto_seguro_safe_driver_prediction', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into Azure ML Dataset and Register into Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load the dataset from the Workspace. Otherwise, create it from the file in the HTTP URL\n",
    "found = False\n",
    "aml_dataset_name = \"porto_seguro_safe_driver_prediction_train\"\n",
    "\n",
    "if aml_dataset_name in ws.datasets.keys(): \n",
    "       found = True\n",
    "       aml_dataset = ws.datasets[aml_dataset_name] \n",
    "       print(\"Dataset loaded from the Workspace\")\n",
    "       \n",
    "if not found:\n",
    "        # Create AML Dataset and register it into Workspace\n",
    "        print(\"Dataset does not exist in the current Workspace. It will be imported and registered.\")\n",
    "        \n",
    "        # Option A: Create AML Dataset from file in AML DataStore\n",
    "        datastore = ws.get_default_datastore()\n",
    "        aml_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('Datasets/porto_seguro_safe_driver_prediction/porto_seguro_safe_driver_prediction_train.csv'))\n",
    "        data_origin_type = 'AMLDataStore'\n",
    "        \n",
    "        # Option B: Create AML Dataset from file in HTTP URL\n",
    "        # data_url = 'https://azmlworkshopdata.blob.core.windows.net/safedriverdata/porto_seguro_safe_driver_prediction_train.csv'\n",
    "        # aml_dataset = Dataset.Tabular.from_delimited_files(data_url)  \n",
    "        # data_origin_type = 'HttpUrl'\n",
    "        \n",
    "        print(aml_dataset)\n",
    "                \n",
    "        #Register Dataset in Workspace\n",
    "        registration_method = 'SDK'  # or 'UI'\n",
    "        aml_dataset = aml_dataset.register(workspace=ws,\n",
    "                                           name=aml_dataset_name,\n",
    "                                           description='Porto Seguro Safe Driver Prediction Train dataset file',\n",
    "                                           tags={'Registration-Method': registration_method, 'Data-Origin-Type': data_origin_type},\n",
    "                                           create_new_version=True)\n",
    "        \n",
    "        print(\"Dataset created from file and registered in the Workspace\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas DataFrame just to sneak peak some data and schema\n",
    "data_df = aml_dataset.to_pandas_dataframe()\n",
    "print(data_df.shape)\n",
    "# print(data_df.describe())\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train and Test AML Tabular Datasets\n",
    "\n",
    "Remote AML Training you need to use AML Datasets, you cannot submit Pandas Dataframes to remote runs of AutoMLConfig.\n",
    "\n",
    "Note that AutoMLConfig below is not using the Test dataset (you only provide a single dataset that will internally be split in validation/train datasets or use cross-validation depending on the size of the dataset. The boundary for that is 20k rows, using cross-validation if less than 20k. This can also be decided by the user.). \n",
    "\n",
    "The Test dataset will be used at the end of the notebook to manually calculate the quality metrics with a dataset not seen by AutoML training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train/test datasets (Test=10%, Train=90%)\n",
    "\n",
    "train_dataset, test_dataset = aml_dataset.random_split(0.9, seed=0)\n",
    "\n",
    "# Use Pandas DF only to check the data\n",
    "train_df = train_dataset.to_pandas_dataframe()\n",
    "test_df = test_dataset.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Remote AML Compute (Existing AML cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "# Define remote compute target to use\n",
    "# Further docs on Remote Compute Target: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-remote\n",
    "\n",
    "# Choose a name for your cluster.\n",
    "amlcompute_cluster_name = \"cpu-cluster\"\n",
    "\n",
    "found = False\n",
    "# Check if this compute target already exists in the workspace.\n",
    "cts = ws.compute_targets\n",
    "\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "     found = True\n",
    "     print('Found existing training cluster.')\n",
    "     # Get existing cluster\n",
    "     # Method 1:\n",
    "     aml_remote_compute = cts[amlcompute_cluster_name]\n",
    "     # Method 2:\n",
    "     # aml_remote_compute = ComputeTarget(ws, amlcompute_cluster_name)\n",
    "    \n",
    "if not found:\n",
    "     print('Creating a new training cluster...')\n",
    "     provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D13_V2\", # for GPU, use \"STANDARD_NC12\"\n",
    "                                                                 #vm_priority = 'lowpriority', # optional\n",
    "                                                                 max_nodes = 5)\n",
    "     # Create the cluster.\n",
    "     aml_remote_compute = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "    \n",
    "print('Checking cluster status...')\n",
    "# Can poll for a minimum number of nodes and for a specific timeout.\n",
    "# If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "aml_remote_compute.wait_for_completion(show_output = True, min_node_count = 0, timeout_in_minutes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For additional details of current AmlCompute status:\n",
    "aml_remote_compute.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Azure AutoML automatically searching for the 'best model' (Best algorithms and best hyper-parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List and select primary metric to drive the AutoML classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train import automl\n",
    "\n",
    "# List of possible primary metrics is here:\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#primary-metric\n",
    "    \n",
    "# Get a list of valid metrics for your given task\n",
    "automl.utilities.get_primary_metrics('classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define AutoML Experiment settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# You can provide additional settings as a **kwargs parameter for the AutoMLConfig object\n",
    "automl_settings = {\n",
    "      \"blacklist_models\":['LogisticRegression', 'ExtremeRandomTrees', 'RandomForest'], \n",
    "      # \"whitelist_models\": ['LightGBM'],\n",
    "      # \"n_cross_validations\": 5,\n",
    "      # \"validation_data\": test_dataset,   # Better to holdout the Test Dataset\n",
    "      \"experiment_exit_score\": 0.7\n",
    "}\n",
    "\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "automl_config = AutoMLConfig(compute_target=aml_remote_compute,\n",
    "                             task='classification',\n",
    "                             primary_metric='AUC_weighted',                           \n",
    "                             training_data=train_dataset, # AML Dataset\n",
    "                             label_column_name=\"target\",                                                    \n",
    "                             enable_early_stopping= True,\n",
    "                             iterations=20,\n",
    "                             max_concurrent_iterations=5,\n",
    "                             experiment_timeout_hours=1,                           \n",
    "                             featurization= 'auto',   # (auto/off) All feature columns in this dataset are numbers, no need to featurize with AML Dataset. \n",
    "                             debug_log='automated_ml_errors.log',\n",
    "                             verbosity= logging.INFO,\n",
    "                             model_explainability=True,\n",
    "                             enable_onnx_compatible_models=False,\n",
    "                             **automl_settings\n",
    "                             )\n",
    "\n",
    "# Explanation of Settings: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train#configure-your-experiment-settings\n",
    "\n",
    "# AutoMLConfig info on: \n",
    "# https://docs.microsoft.com/en-us/python/api/azureml-train-automl-client/azureml.train.automl.automlconfig.automlconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment (on remote AML Compute) with multiple child runs under the covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = \"SDK_remote_porto_seguro_driver_pred\"\n",
    "print(experiment_name)\n",
    "\n",
    "experiment = Experiment(workspace=ws, \n",
    "                        name=experiment_name)\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "            \n",
    "run = experiment.submit(automl_config, show_output=True)\n",
    "\n",
    "print('Manual run timing: --- %s minutes needed for running the whole Remote AutoML Experiment ---' % ((time.time() - start_time)/60))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore results with Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the results of automatic training with a Jupyter widget: https://docs.microsoft.com/en-us/python/api/azureml-widgets/azureml.widgets?view=azure-ml-py\n",
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure Parent Run Time needed for the whole AutoML process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "run_details = run.get_details()\n",
    "\n",
    "# Like: 2020-01-12T23:11:56.292703Z\n",
    "end_time_utc_str = run_details['endTimeUtc'].split(\".\")[0]\n",
    "start_time_utc_str = run_details['startTimeUtc'].split(\".\")[0]\n",
    "timestamp_end = time.mktime(datetime.strptime(end_time_utc_str, \"%Y-%m-%dT%H:%M:%S\").timetuple())\n",
    "timestamp_start = time.mktime(datetime.strptime(start_time_utc_str, \"%Y-%m-%dT%H:%M:%S\").timetuple())\n",
    "\n",
    "parent_run_time = timestamp_end - timestamp_start\n",
    "print('Run Timing: --- %s minutes needed for running the whole Remote AutoML Experiment ---' % (parent_run_time/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the 'Best' Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = run.get_output()\n",
    "print(best_run)\n",
    "print('--------')\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model in Workspace model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model = run.register_model(model_name='porto-seg-automl-remote-compute', \n",
    "                                      description='Porto Seguro Model from plain AutoML in remote AML compute')\n",
    "\n",
    "print(run.model_id)\n",
    "registered_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See files associated with the 'Best run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_run.get_file_names())\n",
    "\n",
    "# best_run.download_file('azureml-logs/70_driver_log.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Test Data: Extract X values (feature columns) from test dataset and convert to NumPi array for predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_test_df = test_df.copy()\n",
    "\n",
    "if 'target' in x_test_df.columns:\n",
    "    y_test_df = x_test_df.pop('target')\n",
    "\n",
    "print(test_df.shape)\n",
    "print(x_test_df.shape)\n",
    "print(y_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the best model making predictions with the test dataset\n",
    "y_predictions = fitted_model.predict(x_test_df)\n",
    "\n",
    "print('30 predictions: ')\n",
    "print(y_predictions[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the predictions' probabilities needed to calculate ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probabilities = fitted_model.predict_proba(x_test_df)\n",
    "print(class_probabilities.shape)\n",
    "\n",
    "print('Some class probabilities...: ')\n",
    "print(class_probabilities[:3])\n",
    "\n",
    "print('Probabilities for class 1:')\n",
    "print(class_probabilities[:,1])\n",
    "\n",
    "print('Probabilities for class 0:')\n",
    "print(class_probabilities[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Evaluating performance is an essential task in machine learning. In this case, because this is a classification problem, the data scientist elected to use an AUC - ROC Curve. When we need to check or visualize the performance of the multi - class classification problem, we use AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve. It is one of the most important evaluation metrics for checking any classification model’s performance.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Oxana_Trifonova/publication/276079439/figure/fig2/AS:614187332034565@1523445079168/An-example-of-ROC-curves-with-good-AUC-09-and-satisfactory-AUC-065-parameters.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 12px; width: 320px; height: 239px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the ROC AUC with probabilities vs. the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ROC AUC *method 1*:')\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test_df, class_probabilities[:,1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print('ROC AUC *method 2*:')\n",
    "print(roc_auc_score(y_test_df, class_probabilities[:,1]))\n",
    "\n",
    "print('ROC AUC Weighted:')\n",
    "print(roc_auc_score(y_test_df, class_probabilities[:,1], average='weighted'))\n",
    "# AUC with plain LightGBM was: 0.6374553321494826 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Accuracy with predictions vs. the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test_df, y_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option A: Load from model .pkl file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model into memory from downloaded file\n",
    "import joblib\n",
    "\n",
    "fitted_model = joblib.load('model.pkl')\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Option B: Load from model registry in Workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from model registry in Workspace\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# model_from_reg = Model(ws, 'porto-seg-automl-remote-compute')\n",
    "\n",
    "name_model_from_plain_automl = 'porto-seg-automl-remote-compute'\n",
    "name_model_from_pipeline_automlstep = 'porto-model-from-automlstep'\n",
    "\n",
    "model_path = Model.get_model_path(name_model_from_pipeline_automlstep, _workspace=ws)\n",
    "fitted_model = joblib.load(model_path)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try model inference with hardcoded input data for the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Dataframe for comparison with hardcoded data\n",
    "# x_test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from Dataframe for comparison with hardcoded data\n",
    "# print(x_test_df.head(1).values)\n",
    "# print(x_test_df.head(1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_data = json.dumps({\n",
    "     'data': [[20,2,1,3,1,0,0,1,0,0,0,0,0,0,0,8,1,0,0,0.6,0.1,0.61745445,6,1,-1,0,1,11,1,1,0,1,99,2,0.31622777,0.6396829,0.36878178,3.16227766,0.2,0.6,0.5,2,2,8,1,8,3,10,3,0,0,10,0,1,0,0,1,0]],\n",
    "     'method': 'predict'  # If you have a classification model, you can get probabilities by changing this to 'predict_proba'.\n",
    " })\n",
    "\n",
    "print(json.loads(raw_data)['data'])\n",
    "\n",
    "numpy_data = np.array(json.loads(raw_data)['data'])\n",
    "\n",
    "df_data = pd.DataFrame(data=numpy_data, columns=['id', 'ps_ind_01', 'ps_ind_02_cat', 'ps_ind_03', 'ps_ind_04_cat',\n",
    "                                               'ps_ind_05_cat', 'ps_ind_06_bin', 'ps_ind_07_bin', 'ps_ind_08_bin',\n",
    "                                               'ps_ind_09_bin', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin',\n",
    "                                               'ps_ind_13_bin', 'ps_ind_14', 'ps_ind_15', 'ps_ind_16_bin',\n",
    "                                               'ps_ind_17_bin', 'ps_ind_18_bin', 'ps_reg_01', 'ps_reg_02', 'ps_reg_03',\n",
    "                                               'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_04_cat',\n",
    "                                               'ps_car_05_cat', 'ps_car_06_cat', 'ps_car_07_cat', 'ps_car_08_cat',\n",
    "                                               'ps_car_09_cat', 'ps_car_10_cat', 'ps_car_11_cat', 'ps_car_11',\n",
    "                                               'ps_car_12', 'ps_car_13', 'ps_car_14', 'ps_car_15', 'ps_calc_01',\n",
    "                                               'ps_calc_02', 'ps_calc_03', 'ps_calc_04', 'ps_calc_05', 'ps_calc_06',\n",
    "                                               'ps_calc_07', 'ps_calc_08', 'ps_calc_09', 'ps_calc_10', 'ps_calc_11',\n",
    "                                               'ps_calc_12', 'ps_calc_13', 'ps_calc_14', 'ps_calc_15_bin',\n",
    "                                               'ps_calc_16_bin', 'ps_calc_17_bin', 'ps_calc_18_bin', 'ps_calc_19_bin',\n",
    "                                               'ps_calc_20_bin'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model\n",
    "y_predictions = fitted_model.predict(df_data) # x_test_df.head(1)\n",
    "y_predictions # Should return a [0] or [1] depending on the prediction result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.5 (aml-conda-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
